import numpy as np
import time
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from mpi4py import MPI
import sys

# Get the MPI Communicator and identify the current process's rank and the total number of processes.
comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()
MASTER_RANK = 0

# =================================================================
# 2. GLOBAL DATA SETUP (GENERATED BY ALL PROCESSES)
# Data is generated once by all processes to be available locally, 
# avoiding expensive network serialization and transfer.
# =================================================================

# Generate and load the data arrays globally.
X, y = make_classification(
    n_samples=100000,
    n_features=200,
    n_informative=10,
    n_redundant=10,
    n_repeated=0,
    n_classes=2,
    n_clusters_per_class=2,
    class_sep=1.2,
    flip_y=0.08,
    weights=[0.5, 0.5],
    random_state=42, # CRUCIAL: Ensures all processes generate identical data
)

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# -------------------------------------------------------------
# 1. Core Training and Evaluation Function
#    This function is executed sequentially by each worker process
#    for its assigned subset of C values.
# -------------------------------------------------------------
def run_trial(C, trial_id):
    """
    Trains a Logistic Regression model for a given C and evaluates it.
    Data is accessed globally.
    """
    
    # Access the globally defined data arrays
    global X_train, y_train, X_val, y_val
    
    # Initialize the model
    clf = LogisticRegression(
        C=C,
        penalty="l1",
        solver="saga",      
        max_iter=3000,
        n_jobs=1,           # Use 1 job per process
        random_state=42
    )

    # Train the model
    clf.fit(X_train, y_train)
    
    # Predict and evaluate
    y_pred = clf.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    
    # Return results including the trial ID for tracking
    return C, acc, trial_id


if __name__ == "__main__":
    
    # -----------------------------
    # 3. Master Process (Rank 0) Logic
    # -----------------------------
    if rank == MASTER_RANK:
        
        rng = np.random.default_rng(0)
        n_trials = 32
        
        # 3.1. Generate all C values
        C_values_full = [10 ** rng.uniform(-5, -1) for _ in range(n_trials)]
        
        # 3.2. Split tasks among all ranks (including master)
        # We need a list of (C, trial_id) tuples to scatter
        tasks = [(C, i) for i, C in enumerate(C_values_full)]
        
        # Calculate how many tasks each worker gets
        # The number of tasks the master (rank 0) handles is included in size
        chunk_size = len(tasks) // size
        remainder = len(tasks) % size
        
        # Build the chunks list for scattering
        chunks = []
        current_index = 0
        for i in range(size):
            # Calculate the size of the current chunk
            current_chunk_size = chunk_size + (1 if i < remainder else 0)
            chunks.append(tasks[current_index:current_index + current_chunk_size])
            current_index += current_chunk_size
        
        # --- Timing Start ---
        start = time.time()
        
    else:
        # Worker processes don't need these master-only variables
        chunks = None
        tasks = None

    # -----------------------------
    # 4. SCATTER: Distribute the C values to all ranks
    # -----------------------------
    # local_tasks will be the subset of (C, trial_id) tuples assigned to this rank
    local_tasks = comm.scatter(chunks, root=MASTER_RANK)

    # -----------------------------
    # 5. EXECUTION: Run the assigned tasks sequentially
    # -----------------------------
    local_results = []
    
    if local_tasks:
        for C, trial_id in local_tasks:
            # Run the actual ML task
            C_res, acc_res, trial_id_res = run_trial(C, trial_id)
            
            # Print intermediate results from workers/master (optional)
            print(f"{trial_id_res:02d}: C={C_res:.3e}, acc={acc_res:.4f}")
            
            local_results.append((C_res, acc_res, trial_id_res))
            
    # -----------------------------
    # 6. GATHER: Collect all results back to the master
    # -----------------------------
    # all_results will be a flat list of results only on the master process (rank 0)
    all_results_list_of_lists = comm.gather(local_results, root=MASTER_RANK)

    # -----------------------------
    # 7. Final Processing (Master Process)
    # -----------------------------
    if rank == MASTER_RANK:
        
        # Flatten the list of lists returned by gather
        final_results = [item for sublist in all_results_list_of_lists for item in sublist]
                
        # 4. Best C
        if final_results:
            best_C, best_acc, best_trial_id = max(final_results, key=lambda t: t[1])
            print("\n--- Best result ---")
            print(f"Trial {best_trial_id:02d}: C={best_C:.3e}, acc={best_acc:.4f}")
        else:
            print("\nNo results found.")
            
        print(f"{time.time()-start}s")
            
        
